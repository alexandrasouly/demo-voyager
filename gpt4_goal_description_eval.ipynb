{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating goal descriptions with GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import os\n",
    "from typing import Dict, Literal, Union\n",
    "\n",
    "from guidance import models, gen, select, system, user, assistant\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to get openai API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    default_path = os.path.expanduser(\"~/.config/openai/key.txt\")\n",
    "    try:\n",
    "        with open(default_path, \"r\") as f:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        raise Exception(\n",
    "            \"Please set OPENAI_API_KEY environment variable to your \"\n",
    "            f\"OpenAI API key (also checked at '{default_path}' and \"\n",
    "            \"could not find it).\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad way of doing it (guidance + GPT 3.5 Turbo Instruct, doesn't work well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SimilarityLevel = Union[Literal[\"Incorrect\"], Literal[\"Correct\"]]\n",
    "similarity_levels: Dict[SimilarityLevel, str] = {\n",
    "    \"Incorrect\": (\n",
    "        \"An agent that tries to achieve the predicted \"\n",
    "        \"goal will fail in achieving the real goal \"\n",
    "        \"because there is a meaningful difference \"\n",
    "        \"between the two goals.\"\n",
    "    ),\n",
    "    \"Correct\": (\n",
    "        \"An agent that tries to achieve the predicted \"\n",
    "        \"goal will succeed in achieving the real goal, \"\n",
    "        \"even if there are slight differences in phrasing \"\n",
    "        \"that does not affect the meaning of the goal.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def get_goal_similarity(llm: models.Model, reference_goal: str, predicted_goal: str) -> SimilarityLevel:\n",
    "    sim_levels = \"\\n\".join(\n",
    "        f\"({num}) {level}: {desc}:\" for num, (level, desc) in enumerate(similarity_levels.items(), start=1)\n",
    "    )\n",
    "    res = llm + f\"\"\"You are a helpful assistant. Your job is to measure the similarity between two predicted goals for a reinforcement learning benchmark environment, where the aim is to manipulate blocks by having a robot push them around in various ways. The best predicted goal is one that tells the agent enough information to achieve the true goal, and no more. Note that the shapes, colors, and locations of blocks are significant. e.g. \"Move the blue block\" is different from \"Move the red block\". There are also goal regions of different colors, and their colors are significant too. Remember, a goal is correct if it would lead to the same outcome, even if the wording is different; a goal is different if it would lead to a different outcome, even if the wording is similar.\n",
    "\n",
    "The {len(similarity_levels)} possible levels of simlarity are:\n",
    "\n",
    "{sim_levels}\n",
    "\n",
    "Here is the ground truth goal:\n",
    "\n",
    "--- BEGIN GROUND TRUTH GOAL ---\n",
    "{reference_goal}\n",
    "--- END GROUND TRUTH GOAL ---\n",
    "\n",
    "Here is the predicted goal:\n",
    "\n",
    "--- BEGIN PREDICTED GOAL ---\n",
    "{predicted_goal}\n",
    "--- END PREDICTED GOAL ---\n",
    "\n",
    "How similar are these goals? Take a deep breath and think step by step (without answering). {gen()}\n",
    "\n",
    "Now, what is the final answer? Select from one of the two answers (spelled in exactly this way: {', '.join(similarity_levels)}). Say only one word, either \"Correct\" or \"Incorrect\".\n",
    "{select(similarity_levels.keys())}\"\"\"\n",
    "    return res\n",
    "\n",
    "get_goal_similarity(\n",
    "    models.OpenAICompletion(\"gpt-3.5-turbo-instruct-0914\", temperature=0),\n",
    "    \"Move the red block to the right of the blue block.\",\n",
    "    \"Move the yellow block forward to the edge of the environment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good way of doing it (GPT4, actually works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_similarity_chat(chat_llm: models.Model, reference_goal: str, predicted_goal: str) -> SimilarityLevel:\n",
    "    sim_levels = \"\\n\".join(\n",
    "        f\"({num}) {level}: {desc}:\" for num, (level, desc) in enumerate(similarity_levels.items(), start=1)\n",
    "    )\n",
    "    with system():\n",
    "        res = chat_llm + f\"\"\"You are a helpful assistant. Your job is to measure the similarity between two predicted goals for a reinforcement learning benchmark environment, where the aim is to manipulate blocks by having a robot push them around in various ways. The best predicted goal is one that tells the agent enough information to achieve the true goal, and no more. Note that the shapes, colors, and locations of blocks are significant. e.g. \"Move the blue block\" is different from \"Move the red block\". There are also goal regions of different colors, and their colors are significant too. Remember, a goal is correct if it would lead to the same outcome, even if the wording is different; a goal is different if it would lead to a different outcome, even if the wording is similar.\n",
    "\n",
    "The {len(similarity_levels)} possible levels of simlarity are:\n",
    "\n",
    "{sim_levels}\n",
    "\n",
    "Here is the ground truth goal:\n",
    "\n",
    "--- BEGIN GROUND TRUTH GOAL ---\n",
    "{reference_goal}\n",
    "--- END GROUND TRUTH GOAL ---\n",
    "\n",
    "Here is the predicted goal:\n",
    "\n",
    "--- BEGIN PREDICTED GOAL ---\n",
    "{predicted_goal}\n",
    "--- END PREDICTED GOAL ---\n",
    "\n",
    "How similar are these goals? Take a deep breath and reason step by step. Reason without answering first. Be concise.\"\"\"\n",
    "\n",
    "    # do think step-by-step step\n",
    "    with assistant():\n",
    "        res = res + gen()\n",
    "\n",
    "    with user():\n",
    "        res = res + f\"\"\"Now, what is the final answer? Select from one of the two answers (spelled in exactly this way: {', '.join(similarity_levels)}). Say only one of the options and nothing else.\"\"\"\n",
    "\n",
    "    # final answer\n",
    "    with assistant():\n",
    "        res = res + select(similarity_levels.keys(), name=\"sim_level\")\n",
    "\n",
    "    return res.get(\"sim_level\")\n",
    "\n",
    "get_goal_similarity_chat(\n",
    "    models.OpenAIChat(\"gpt-4-1106-preview\", temperature=0),\n",
    "    \"Move the red block to the right of the blue block.\",\n",
    "    # \"Move the yellow block forward to the edge of the environment\",\n",
    "    \"Move the red block such that the blue block is to its left.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magical-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
